When you put all this together, you find that all these possibilites on the Gaussian over here, link to a Gaussian that looks like this.

And this is a really interesting 2-dimensional Gaussian, which you should really think about. Clearly, if you were to project this Gaussian uncertainty into the space of possible locations, I can't predict a thing. It's impossible to predict where the object is. And the reason is, I don't know the velocity. Also, clearly if I project this Gaussian into the space of x dot, then it is impossible to say what the velocity is. A single observation or single prediction is insufficient to make that observation.

However, what we know is our location is correlated to the velocity. The faster I move, the further on the right is the location. And this Gaussian expresses this.

If I, for example, figured out that my velocity was 2, then I was able, under this Gaussian, to really nail that my location is 3. That is really remarkable.

You still haven't figured out where you are, and we haven't figured out how fast we're moving, but we've learned so much about the relation of these 2 things with this Gaussian.

To understand how powerful this is, let's now fold in the second observation at time t = 2. This observation tells us nothing about the velocity and only something about the location. So if I were to draw this as a Gaussian, it's a Gaussian just like this, which says something about the location but not about the velocity. But if we now multiply by prior from the prediction step with the measurement probability, then miraculously, I get a Gaussian that sits right over here. And this Gaussian now has a really good estimate what my velocity is and a really good estimate of where I am.

If I take this Gaussian, and predict 1 step forward, then I find myself right over here. That's exactly the effect we have over here. As I update this I get a Gaussian like this, I predict right over here. Think about this. This is really deep insight about how Kalman filters work. In particular, we've only been able to observe 1 variable. We've been able to measure observation to infer this other variable, and the way we've been able to infer it is that there's a set of physical equations which say that my location, after a time step, is my old location plus my velocity.

And this set of equations has been able to propagate constrains from subsequent measurements back to this unobservable variable, velocity, so we are able to estimate the velocity as well. This is really key to understanding Kalman filter. It is key to understand how a Google self-driving car estimates the locations of other cars, and is able to make predictions even if it's unable to measure velocity directly.

So there's a big lesson here. The variables of a Kalman filter, they're often called states because they reflect states of the physical world like where is the other car and the fastest moving. They separate into 2 subsets: the observables, like the momentary location, and the hidden, which in our example is the velocity, which I can never directly observe.

But because those 2 things interact, subsequent observations of the observable variables give us information about these hidden variables, so we can also estimate what these hidden variables are.

So from multiple observations of the places of the object, the location, we can estimate how fast it's moving.

That is actually true for all of the different filters but because Kalman filters happen to be very efficient to calculate, when we have a problem like this, you tend to often use just a Kalman filter.
